{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCtiLheWdLTS"
      },
      "source": [
        "# Global Tweet Sentiment Analysis - Mood of the World"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jl2PbgOKdUia"
      },
      "source": [
        "# Install Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsXufA_7XySO"
      },
      "outputs": [],
      "source": [
        "!pip install tweepy textblob plotly pandas numpy wordcloud matplotlib seaborn python-dotenv --quiet\n",
        "!python -m textblob.download_corpora"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwbnEPFwdbDO"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtEaBILWX8nv"
      },
      "outputs": [],
      "source": [
        "import tweepy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from textblob import TextBlob\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.offline as pyo\n",
        "from datetime import datetime, timedelta\n",
        "import re\n",
        "import json\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "pyo.init_notebook_mode(connected=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOq4Gvjydn4g"
      },
      "source": [
        "# Twitter API Configuration\n",
        "## Note : get API from https://developer.twitter.com/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfCHLwrwd_Mn"
      },
      "outputs": [],
      "source": [
        "class TwitterConfig:\n",
        "    def __init__(self):\n",
        "        # Replace with your actual API credentials\n",
        "        self.API_KEY = \"YOUR_API_KEY\"\n",
        "        self.API_SECRET = \"YOUR_API_SECRET\"\n",
        "        self.ACCESS_TOKEN = \"YOUR_ACCESS_TOKEN\"\n",
        "        self.ACCESS_TOKEN_SECRET = \"YOUR_ACCESS_TOKEN_SECRET\"\n",
        "        self.BEARER_TOKEN = \"YOUR_BEARER_TOKEN\"\n",
        "\n",
        "    def get_api_v1(self):\n",
        "        \"\"\"Get Twitter API v1.1 client\"\"\"\n",
        "        auth = tweepy.OAuthHandler(self.API_KEY, self.API_SECRET)\n",
        "        auth.set_access_token(self.ACCESS_TOKEN, self.ACCESS_TOKEN_SECRET)\n",
        "        return tweepy.API(auth, wait_on_rate_limit=True)\n",
        "\n",
        "    def get_api_v2(self):\n",
        "        \"\"\"Get Twitter API v2 client\"\"\"\n",
        "        return tweepy.Client(\n",
        "            bearer_token=self.BEARER_TOKEN,\n",
        "            consumer_key=self.API_KEY,\n",
        "            consumer_secret=self.API_SECRET,\n",
        "            access_token=self.ACCESS_TOKEN,\n",
        "            access_token_secret=self.ACCESS_TOKEN_SECRET,\n",
        "            wait_on_rate_limit=True\n",
        "        )\n",
        "config = TwitterConfig()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkgYKLtdeLNj"
      },
      "source": [
        "# Data Collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZa1gmgheKoL"
      },
      "outputs": [],
      "source": [
        "class GlobalTweetCollector:\n",
        "    def __init__(self, api_v1, api_v2):\n",
        "        self.api_v1 = api_v1\n",
        "        self.api_v2 = api_v2\n",
        "        self.tweets_data = []\n",
        "\n",
        "    def clean_tweet(self, text):\n",
        "        \"\"\"Clean tweet text for better sentiment analysis\"\"\"\n",
        "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "        text = re.sub(r'@\\w+|#\\w+', '', text)\n",
        "        text = ' '.join(text.split())\n",
        "        return text.strip()\n",
        "\n",
        "    def get_sentiment(self, text):\n",
        "        \"\"\"Analyze sentiment using TextBlob\"\"\"\n",
        "        try:\n",
        "            blob = TextBlob(text)\n",
        "            polarity = blob.sentiment.polarity\n",
        "            subjectivity = blob.sentiment.subjectivity\n",
        "            if polarity > 0.1:\n",
        "                sentiment = 'Positive'\n",
        "            elif polarity < -0.1:\n",
        "                sentiment = 'Negative'\n",
        "            else:\n",
        "                sentiment = 'Neutral'\n",
        "\n",
        "            return {\n",
        "                'sentiment': sentiment,\n",
        "                'polarity': polarity,\n",
        "                'subjectivity': subjectivity\n",
        "            }\n",
        "        except:\n",
        "            return {\n",
        "                'sentiment': 'Neutral',\n",
        "                'polarity': 0.0,\n",
        "                'subjectivity': 0.0\n",
        "            }\n",
        "\n",
        "    def collect_global_tweets(self, keywords=None, count=1000):\n",
        "        \"\"\"Collect tweets from around the world\"\"\"\n",
        "        if keywords is None:\n",
        "            keywords = [\n",
        "                \"good morning\", \"how are you\", \"feeling\", \"mood\", \"happy\",\n",
        "                \"sad\", \"excited\", \"worried\", \"grateful\", \"blessed\",\n",
        "                \"monday\", \"tuesday\", \"wednesday\", \"thursday\", \"friday\",\n",
        "                \"weekend\", \"today\", \"life\", \"work\", \"family\"\n",
        "            ]\n",
        "\n",
        "        print(f\"Starting global tweet collection...\")\n",
        "\n",
        "        for keyword in keywords:\n",
        "            try:\n",
        "                print(f\"Searching for: '{keyword}'\")\n",
        "                tweets = tweepy.Paginator(\n",
        "                    self.api_v2.search_recent_tweets,\n",
        "                    query=f\"{keyword} -is:retweet lang:en\",\n",
        "                    tweet_fields=['created_at', 'author_id', 'public_metrics', 'geo'],\n",
        "                    max_results=100,\n",
        "                    limit=10\n",
        "                ).flatten(limit=count // len(keywords))\n",
        "\n",
        "                for tweet in tweets:\n",
        "                    if tweet.text:\n",
        "                        cleaned_text = self.clean_tweet(tweet.text)\n",
        "                        if len(cleaned_text) > 10:\n",
        "                            sentiment_data = self.get_sentiment(cleaned_text)\n",
        "\n",
        "                            tweet_data = {\n",
        "                                'id': tweet.id,\n",
        "                                'text': cleaned_text,\n",
        "                                'original_text': tweet.text,\n",
        "                                'created_at': tweet.created_at,\n",
        "                                'author_id': tweet.author_id,\n",
        "                                'keyword': keyword,\n",
        "                                'sentiment': sentiment_data['sentiment'],\n",
        "                                'polarity': sentiment_data['polarity'],\n",
        "                                'subjectivity': sentiment_data['subjectivity'],\n",
        "                                'retweet_count': tweet.public_metrics['retweet_count'] if tweet.public_metrics else 0,\n",
        "                                'like_count': tweet.public_metrics['like_count'] if tweet.public_metrics else 0,\n",
        "                            }\n",
        "\n",
        "                            self.tweets_data.append(tweet_data)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error collecting tweets for '{keyword}': {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        print(f\"Collected {len(self.tweets_data)} tweets total\")\n",
        "        return pd.DataFrame(self.tweets_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aI1zPxCvfL4a"
      },
      "source": [
        "# Initialize APIs and Collect Data or Sample Data for Demonstration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNBAgeuWfMAq"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    api_v1 = config.get_api_v1()\n",
        "    api_v2 = config.get_api_v2()\n",
        "\n",
        "    print(\"Testing Twitter API connection...\")\n",
        "    me = api_v1.verify_credentials()\n",
        "    if me:\n",
        "        print(f\"Connected as: @{me.screen_name}\")\n",
        "    else:\n",
        "        print(\"API connection failed.\")\n",
        "    collector = GlobalTweetCollector(api_v1, api_v2)\n",
        "    df_tweets = collector.collect_global_tweets(count=2000)\n",
        "\n",
        "    if len(df_tweets) > 0:\n",
        "        print(f\"Dataset shape: {df_tweets.shape}\")\n",
        "        print(\"\\n Dataset info:\")\n",
        "        print(df_tweets.info())\n",
        "        print(f\"\\n Sentiment distribution:\")\n",
        "        print(df_tweets['sentiment'].value_counts())\n",
        "    else:\n",
        "        print(\"No tweets collected. Check your API credentials.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error: {str(e)}\")\n",
        "\n",
        "    print(\"\\n Creating sample data for demonstration...\")\n",
        "    sample_data = {\n",
        "        'text': [\n",
        "            'Having a great day today!',\n",
        "            'Feeling a bit down lately',\n",
        "            'Just another normal Monday',\n",
        "            'So excited for the weekend!',\n",
        "            'Work is really stressing me out',\n",
        "            'Grateful for my family and friends',\n",
        "            'Weather is perfect today',\n",
        "            'This week has been challenging',\n",
        "            'Looking forward to vacation',\n",
        "            'Love spending time with loved ones'\n",
        "        ] * 50,\n",
        "        'sentiment': ['Positive', 'Negative', 'Neutral', 'Positive', 'Negative'] * 100,\n",
        "        'polarity': np.random.normal(0, 0.3, 500),\n",
        "        'subjectivity': np.random.uniform(0, 1, 500),\n",
        "        'created_at': pd.date_range(start='2024-01-01', periods=500, freq='H'),\n",
        "        'keyword': np.random.choice(['mood', 'feeling', 'today', 'happy', 'work'], 500)\n",
        "    }\n",
        "    df_tweets = pd.DataFrame(sample_data)\n",
        "    print(f\"Sample dataset created with {len(df_tweets)} tweets\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6Hh1nSsgG0j"
      },
      "source": [
        "# Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfAEAFcbgG73"
      },
      "outputs": [],
      "source": [
        "class SentimentAnalyzer:\n",
        "    def __init__(self, df):\n",
        "        self.df = df.copy()\n",
        "        self.prepare_data()\n",
        "\n",
        "    def prepare_data(self):\n",
        "        \"\"\"Prepare data for analysis\"\"\"\n",
        "        if 'created_at' in self.df.columns:\n",
        "            self.df['created_at'] = pd.to_datetime(self.df['created_at'])\n",
        "            self.df['hour'] = self.df['created_at'].dt.hour\n",
        "            self.df['day_of_week'] = self.df['created_at'].dt.day_name()\n",
        "            self.df['date'] = self.df['created_at'].dt.date\n",
        "\n",
        "    def get_sentiment_summary(self):\n",
        "        \"\"\"Get overall sentiment statistics\"\"\"\n",
        "        total_tweets = len(self.df)\n",
        "        sentiment_counts = self.df['sentiment'].value_counts()\n",
        "\n",
        "        summary = {\n",
        "            'total_tweets': total_tweets,\n",
        "            'positive_pct': (sentiment_counts.get('Positive', 0) / total_tweets) * 100,\n",
        "            'negative_pct': (sentiment_counts.get('Negative', 0) / total_tweets) * 100,\n",
        "            'neutral_pct': (sentiment_counts.get('Neutral', 0) / total_tweets) * 100,\n",
        "            'avg_polarity': self.df['polarity'].mean(),\n",
        "            'avg_subjectivity': self.df['subjectivity'].mean()\n",
        "        }\n",
        "\n",
        "        return summary\n",
        "\n",
        "    def get_hourly_sentiment(self):\n",
        "        \"\"\"Analyze sentiment by hour of day\"\"\"\n",
        "        hourly_sentiment = self.df.groupby(['hour', 'sentiment']).size().unstack(fill_value=0)\n",
        "        hourly_polarity = self.df.groupby('hour')['polarity'].mean()\n",
        "\n",
        "        return hourly_sentiment, hourly_polarity\n",
        "\n",
        "    def get_daily_sentiment(self):\n",
        "        \"\"\"Analyze sentiment by day of week\"\"\"\n",
        "        daily_sentiment = self.df.groupby(['day_of_week', 'sentiment']).size().unstack(fill_value=0)\n",
        "        daily_polarity = self.df.groupby('day_of_week')['polarity'].mean()\n",
        "\n",
        "        return daily_sentiment, daily_polarity\n",
        "\n",
        "    def get_keyword_sentiment(self):\n",
        "        \"\"\"Analyze sentiment by keyword\"\"\"\n",
        "        if 'keyword' in self.df.columns:\n",
        "            keyword_sentiment = self.df.groupby(['keyword', 'sentiment']).size().unstack(fill_value=0)\n",
        "            keyword_polarity = self.df.groupby('keyword')['polarity'].mean().sort_values(ascending=False)\n",
        "            return keyword_sentiment, keyword_polarity\n",
        "        return None, None\n",
        "\n",
        "analyzer = SentimentAnalyzer(df_tweets)\n",
        "summary = analyzer.get_sentiment_summary()\n",
        "\n",
        "print(\"Mood of the World SUMMARY\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"Total Tweets Analyzed: {summary['total_tweets']:,}\")\n",
        "print(f\"Positive Sentiment: {summary['positive_pct']:.1f}%\")\n",
        "print(f\"Negative Sentiment: {summary['negative_pct']:.1f}%\")\n",
        "print(f\"Neutral Sentiment: {summary['neutral_pct']:.1f}%\")\n",
        "print(f\"Average Polarity: {summary['avg_polarity']:.3f}\")\n",
        "print(f\"Average Subjectivity: {summary['avg_subjectivity']:.3f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "NwbnEPFwdbDO",
        "IOq4Gvjydn4g",
        "MkgYKLtdeLNj",
        "aI1zPxCvfL4a"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
